# SatMLCompetition-DocumentIntelligence (Track 2)

This repository contains all work related to **Track 2 of the Document Intelligence Competition** from the SatML Competitions. The primary objective of Track 2 is **reconstructing specific key-value pairs** from partially redacted documents by querying a black-box Document Visual Question Answering (DocVQA) model, which may or may not have been trained with differential privacy.

---

## Table of Contents

1. [Project Overview](#project-overview)  
2. [Repository Structure](#repository-structure)  
3. [Setup and Installation](#setup-and-installation)  
4. [Usage Instructions](#usage-instructions)  
   - [1. Generating Queries (`assemble.py`)](#1-generating-queries-assemblepy)  
   - [2. Querying the Black-Box Model (`client.py`)](#2-querying-the-black-box-model-clientpy)  
5. [Key Files and Scripts](#key-files-and-scripts)  
---

## Project Overview

### Context

- **Competition**: SatML “Document Intelligence” challenge.  
- **Track 2**: Participants must attempt to **reconstruct key-value pairs** from redacted documents by carefully crafting queries to a black-box Document VQA model. This model is only accessible via an API, and daily queries are limited.

### Goals

1. **Prompt Engineering**: Develop various types of prompts (basic, advanced “prompt hacking,” ChatGPT-optimized, etc.) and compare their effectiveness in extracting redacted information.  
2. **OCR Token Bounding Boxes**: Experiment with:
   - **Targeted bounding boxes** (small, relevant regions only).  
   - **Full-document bounding boxes** (the entire page).  
3. **Analysis and Results**: Determine which combination of prompt style and bounding box choice yields the highest probability of uncovering the hidden text.

### High-Level Approach

1. **Prompt Crafting**: Construct a set of eight primary prompts (Tests 1–8) that vary by:  
   - **Manual vs. ChatGPT-optimized** phrasing  
   - **Basic vs. Advanced** (a.k.a. “prompt hacking”)  
   - **Targeted vs. Full OCR** token bounding  
2. **Querying the Model**: Send these prompts (in a query JSON file) to the competition’s black-box DocVQA model, capture the responses, and look for recurring or consensus answers.  
3. **Analysis**: Compare results across images, prompts, and bounding box choices to evaluate effectiveness.

---

## Repository Structure

- **`assemble.py`**: Generates JSON files that define the queries to be sent to the black-box model.  
- **`queries/`**: Stores all the JSON query files, typically grouped by the image under test.  
- **`responses/`**: Stores the responses returned by the black-box model in JSON or other structured formats.  
- **`api_red/client.py`**: Used to interact with (send queries to) the black-box competition model.  
- **`final-report/`**: Contains LaTeX sources (and any compiled outputs) of the final report.

---

## Setup and Installation

1. **Clone this repository**:
   ```bash
   git clone https://github.com/vihanpatil/SatMLCompetition-DocumentIntelligence.git
   cd SatMLCompetition-DocumentIntelligence

2. **(Optional) Create and activate a Python virtual environment:**
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate

3. **Install dependencies (if you have a requirements.txt or similar):**
   ```bash
   pip install [requirements]
install all relevant packages (e.g., requests, etc.) manually.

4. **Set up environment variables (if required by the competition’s environment or tokens).**

## Usage Instructions
1. **Generating Queries (assemble.py)**
Use assemble.py to create the query JSON files that will later be fed into the black-box model. For example:

   ```bash
   python3 assemble.py \
      --url "/path/to/your/eval_images/Interlock_0_eval.jpg"
- --url: The path (or URL) to the image you want to analyze. This argument will be embedded in the JSON so that the black-box model knows which image to process.
- Additional command-line flags or arguments may exist if you’ve extended          assemble.py. Refer to the script’s internal docstrings or --help for more details.
Note: This step does not send the query to the server. It only creates a query JSON file (e.g., queries/image5/subtotal1.json).

2. Querying the Black-Box Model (client.py)
Once you have your query JSON file, you can send it to the competition server:

   ```bash
   python3 api_red/client.py \
       --token <YOUR_COMPETITION_TOKEN> \
       --query_path queries/image5/subtotal1.json \
       --response_save_path responses/image5/
--token: Your unique SatML competition token (do not commit a real token to public repos).
--query_path: The path to the JSON file generated by assemble.py.
--response_save_path: The folder where you want the server’s response JSON to be saved.
**Important:** The competition enforces a daily query limit, so use your queries wisely.

### Key Files and Scripts
**assemble.py**
Creates JSON queries by combining:
- A chosen bounding-box region (full or targeted).
- Your desired prompt style (basic, advanced, ChatGPT-optimized, etc.).
- References to the image(s) in question.

**client.py** (located in api_red/client.py)
Handles the actual network call to the black-box model endpoint.

- Takes in a query JSON file and outputs a response JSON file.

**queries/**
- Subdirectories contain the raw queries for each image (e.g., image5/subtotal1.json might define a request to retrieve the “Subtotal” field from image5).

**responses/**
- Houses all JSON results from the black-box model, mirroring the structure of queries/.

**CSE_233_Final_Report.pdf**
- The detailed LaTeX final report with methodology, references, tables of results, and more.

